name: Performance Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - load
          - stress
          - memory

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        node-version: [18.x, 20.x]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        
    - name: Install dependencies
      run: |
        npm ci
        cd frontend && npm ci
        cd ../backend && pip install -r requirements.txt
        
    - name: Install Puppeteer dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          ca-certificates \
          fonts-liberation \
          libappindicator3-1 \
          libasound2 \
          libatk-bridge2.0-0 \
          libatk1.0-0 \
          libc6 \
          libcairo2 \
          libcups2 \
          libdbus-1-3 \
          libexpat1 \
          libfontconfig1 \
          libgbm1 \
          libgcc1 \
          libglib2.0-0 \
          libgtk-3-0 \
          libnspr4 \
          libnss3 \
          libpango-1.0-0 \
          libpangocairo-1.0-0 \
          libstdc++6 \
          libx11-6 \
          libx11-xcb1 \
          libxcb1 \
          libxcomposite1 \
          libxcursor1 \
          libxdamage1 \
          libxext6 \
          libxfixes3 \
          libxi6 \
          libxrandr2 \
          libxrender1 \
          libxss1 \
          libxtst6 \
          lsb-release \
          wget \
          xdg-utils
          
    - name: Build frontend
      run: |
        cd frontend
        npm run build
      env:
        CI: false
        
    - name: Start backend server
      run: |
        cd backend
        python server.py &
        sleep 5
        
    - name: Run performance tests
      run: |
        cd frontend
        node src/components/layout/MissionControl/LODSystem/__tests__/performance/ci-performance-test.js
      env:
        PERF_TEST_URL: http://localhost:3000
        PERF_OUTPUT_DIR: ./performance-results
        PERF_SCREENSHOT_DIR: ./performance-screenshots
        PERF_FAIL_ON_REGRESSION: ${{ github.event_name == 'pull_request' && 'true' || 'false' }}
        PERF_REGRESSION_THRESHOLD: '0.1'
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results-${{ matrix.node-version }}
        path: |
          frontend/performance-results/
          frontend/performance-screenshots/
          frontend/benchmark-history.json
          
    - name: Upload performance report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-report-${{ matrix.node-version }}
        path: frontend/performance-results/performance-report-*.md
        
    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: |
          frontend/performance-results/performance-junit.xml
        check_name: Performance Test Results (Node ${{ matrix.node-version }})
        
    - name: Comment PR with results
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request'
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Find the latest report
          const resultsDir = './frontend/performance-results';
          const files = fs.readdirSync(resultsDir);
          const reportFile = files.find(f => f.startsWith('performance-report-'));
          
          if (reportFile) {
            const report = fs.readFileSync(path.join(resultsDir, reportFile), 'utf8');
            
            // Create collapsible comment
            const comment = `<details>
            <summary>ðŸ“Š Performance Test Results (Node ${{ matrix.node-version }})</summary>
            
            ${report}
            </details>`;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
          
  performance-comparison:
    needs: performance-test
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Download current results
      uses: actions/download-artifact@v4
      with:
        pattern: performance-results-*
        merge-multiple: true
        
    - name: Checkout base branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.base_ref }}
        path: base
        
    - name: Download base results
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: performance-tests.yml
        branch: ${{ github.base_ref }}
        name: performance-results-18.x
        path: base-results
      continue-on-error: true
      
    - name: Compare performance
      run: |
        # Simple comparison script
        if [ -d "base-results" ]; then
          echo "## Performance Comparison" > comparison.md
          echo "" >> comparison.md
          echo "Comparing against base branch: ${{ github.base_ref }}" >> comparison.md
          # Add comparison logic here
        else
          echo "No base results found for comparison" > comparison.md
        fi
        
    - name: Comment comparison
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('comparison.md', 'utf8');
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comparison
          });
          
  lighthouse-performance:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18.x'
        
    - name: Install and build
      run: |
        npm ci
        cd frontend && npm ci && npm run build
        
    - name: Run Lighthouse CI
      uses: treosh/lighthouse-ci-action@v11
      with:
        urls: |
          http://localhost:3000
          http://localhost:3000/mission-control
        uploadArtifacts: true
        temporaryPublicStorage: true
        configPath: './.lighthouserc.js'